{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create documents\n",
    "depression_set = []\n",
    "for i in xrange(781):\n",
    "    f = open('depress_corpus/corpus_'+str(i))\n",
    "    line = \"\"\n",
    "    for j in f:\n",
    "        line += j.decode('utf-8')\n",
    "    depression_set.append(line)\n",
    "    f.close()\n",
    "control_set = []\n",
    "for i in xrange(824):\n",
    "    f = open('other_corpus/corpus_'+str(i))\n",
    "    line = \"\"\n",
    "    for j in f:\n",
    "        line += j.decode('utf-8')\n",
    "    control_set.append(line)\n",
    "    f.close()\n",
    "doc_set = depression_set + control_set\n",
    "label = [1]*781 + [0]*824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop and len(i)!=1]\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "data = []\n",
    "row = []\n",
    "col = []\n",
    "for (N,c) in enumerate(corpus):\n",
    "    for i in c:\n",
    "        data.append(i[1])\n",
    "        row.append(N)\n",
    "        col.append(i[0])\n",
    "BoW_feature = csr_matrix((data,(row,col))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_lda = ldamodel[corpus]\n",
    "data = []\n",
    "row = []\n",
    "col = []\n",
    "for (N,doc) in enumerate(doc_lda):\n",
    "    for i in doc:\n",
    "        data.append(i[1])\n",
    "        row.append(N)\n",
    "        col.append(i[0])\n",
    "LDA_feature = csr_matrix((data,(row,col))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1605, 9453)\n",
      "(1605, 20)\n"
     ]
    }
   ],
   "source": [
    "print BoW_feature.shape\n",
    "print LDA_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.save('bow_feature.npy',BoW_feature)\n",
    "numpy.save('lda_feature.npy',BoW_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.014*\"go\" + 0.008*\"one\" + 0.006*\"time\" + 0.006*\"get\" + 0.005*\"know\" + 0.005*\"like\" + 0.004*\"could\" + 0.004*\"work\" + 0.004*\"think\" + 0.004*\"would\"'), (1, u'0.015*\"feel\" + 0.010*\"like\" + 0.007*\"would\" + 0.007*\"go\" + 0.006*\"time\" + 0.006*\"get\" + 0.005*\"know\" + 0.005*\"one\" + 0.005*\"someth\" + 0.005*\"need\"'), (2, u'0.011*\"like\" + 0.011*\"get\" + 0.006*\"year\" + 0.006*\"back\" + 0.006*\"feel\" + 0.006*\"time\" + 0.006*\"day\" + 0.006*\"tri\" + 0.005*\"would\" + 0.005*\"want\"'), (3, u'0.008*\"go\" + 0.008*\"time\" + 0.007*\"one\" + 0.006*\"get\" + 0.006*\"like\" + 0.005*\"say\" + 0.005*\"could\" + 0.005*\"feel\" + 0.005*\"help\" + 0.005*\"year\"'), (4, u'0.009*\"go\" + 0.008*\"like\" + 0.008*\"would\" + 0.007*\"back\" + 0.006*\"look\" + 0.006*\"know\" + 0.006*\"get\" + 0.006*\"year\" + 0.006*\"make\" + 0.005*\"want\"'), (5, u'0.010*\"go\" + 0.009*\"peopl\" + 0.009*\"want\" + 0.008*\"would\" + 0.008*\"get\" + 0.008*\"feel\" + 0.007*\"time\" + 0.007*\"year\" + 0.007*\"like\" + 0.006*\"depress\"'), (6, u'0.013*\"get\" + 0.011*\"feel\" + 0.010*\"go\" + 0.010*\"time\" + 0.010*\"like\" + 0.008*\"day\" + 0.008*\"depress\" + 0.007*\"know\" + 0.007*\"want\" + 0.007*\"peopl\"'), (7, u'0.021*\"like\" + 0.017*\"feel\" + 0.012*\"realli\" + 0.011*\"time\" + 0.009*\"depress\" + 0.008*\"peopl\" + 0.006*\"one\" + 0.006*\"want\" + 0.006*\"would\" + 0.006*\"get\"'), (8, u'0.012*\"feel\" + 0.010*\"depress\" + 0.009*\"like\" + 0.008*\"day\" + 0.008*\"would\" + 0.008*\"one\" + 0.008*\"time\" + 0.007*\"life\" + 0.006*\"go\" + 0.006*\"get\"'), (9, u'0.010*\"get\" + 0.007*\"want\" + 0.007*\"go\" + 0.006*\"day\" + 0.006*\"even\" + 0.006*\"one\" + 0.006*\"fuck\" + 0.006*\"someth\" + 0.006*\"like\" + 0.005*\"back\"'), (10, u'0.015*\"feel\" + 0.015*\"want\" + 0.015*\"like\" + 0.010*\"get\" + 0.010*\"peopl\" + 0.007*\"day\" + 0.007*\"thing\" + 0.006*\"tri\" + 0.006*\"time\" + 0.006*\"even\"'), (11, u'0.009*\"time\" + 0.007*\"one\" + 0.006*\"would\" + 0.006*\"get\" + 0.006*\"could\" + 0.006*\"know\" + 0.005*\"tri\" + 0.005*\"like\" + 0.005*\"much\" + 0.005*\"make\"'), (12, u'0.014*\"like\" + 0.009*\"feel\" + 0.009*\"even\" + 0.007*\"get\" + 0.007*\"time\" + 0.006*\"day\" + 0.006*\"go\" + 0.005*\"com\" + 0.005*\"want\" + 0.005*\"http\"'), (13, u'0.012*\"would\" + 0.011*\"like\" + 0.010*\"feel\" + 0.010*\"want\" + 0.008*\"know\" + 0.007*\"time\" + 0.006*\"think\" + 0.006*\"get\" + 0.006*\"life\" + 0.006*\"go\"'), (14, u'0.008*\"would\" + 0.007*\"like\" + 0.007*\"know\" + 0.007*\"com\" + 0.007*\"want\" + 0.007*\"get\" + 0.007*\"http\" + 0.006*\"peopl\" + 0.006*\"life\" + 0.006*\"tri\"'), (15, u'0.007*\"time\" + 0.006*\"peopl\" + 0.006*\"one\" + 0.006*\"feel\" + 0.005*\"like\" + 0.005*\"even\" + 0.004*\"know\" + 0.004*\"get\" + 0.004*\"look\" + 0.004*\"thank\"'), (16, u'0.011*\"time\" + 0.010*\"want\" + 0.009*\"feel\" + 0.009*\"go\" + 0.007*\"like\" + 0.007*\"get\" + 0.006*\"life\" + 0.006*\"would\" + 0.006*\"still\" + 0.006*\"way\"'), (17, u'0.011*\"one\" + 0.010*\"depress\" + 0.010*\"like\" + 0.009*\"know\" + 0.008*\"get\" + 0.008*\"go\" + 0.008*\"peopl\" + 0.007*\"make\" + 0.006*\"realli\" + 0.005*\"thing\"'), (18, u'0.018*\"feel\" + 0.016*\"like\" + 0.011*\"get\" + 0.009*\"peopl\" + 0.007*\"depress\" + 0.007*\"see\" + 0.007*\"one\" + 0.006*\"think\" + 0.006*\"day\" + 0.006*\"know\"'), (19, u'0.014*\"know\" + 0.011*\"feel\" + 0.011*\"get\" + 0.010*\"depress\" + 0.009*\"time\" + 0.009*\"like\" + 0.008*\"peopl\" + 0.007*\"make\" + 0.006*\"want\" + 0.006*\"take\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=20, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.010000000068013774),\n",
       " (1, 0.010000000086333411),\n",
       " (2, 0.010000000132431021),\n",
       " (3, 0.010000000167327524),\n",
       " (4, 0.010000000211210242),\n",
       " (5, 0.01000000012438532),\n",
       " (6, 0.010000000180284314),\n",
       " (7, 0.010000000165587277),\n",
       " (8, 0.010000000173138613),\n",
       " (9, 0.010000000131704417),\n",
       " (10, 0.010000000159253305),\n",
       " (11, 0.010000000126559732),\n",
       " (12, 0.010000000121553594),\n",
       " (13, 0.010000000208291313),\n",
       " (14, 0.010000000146996247),\n",
       " (15, 0.010000000068586274),\n",
       " (16, 0.010000000522428148),\n",
       " (17, 0.010000000162195013),\n",
       " (18, 0.80999999685326807),\n",
       " (19, 0.010000000190452376)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
